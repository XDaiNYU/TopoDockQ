{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998b2106",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Download the necessary feature and model files from the Zenodo repository\n",
    "\n",
    "### `processed_data.zip` (661.7 MB) contains:\n",
    "- **`singlePPD_full_bins_features.csv`**: Generated TopoDockQ features for model training, validation, and testing in the SinglePPD dataset.\n",
    "- **`singlePPD_DockQ.csv`**: Contains the full set of models in the SinglePPD dataset, including all 50 predictions per complex generated by AlphaFold2-Multimer.\n",
    "- **`singlePPD_filtered_DockQ.csv`**: A curated subset in which mutually highly similar models (DockQ \u2265 0.98) have been removed from the training set.  \n",
    "  Only the training set is filtered; validation and test sets remain unfiltered to reflect real-world model selection scenarios.\n",
    "\n",
    "\ud83d\udcc2 **Installation**: Extract this zip file to the `./data/processed_data/` folder.\n",
    "\n",
    "---\n",
    "\n",
    "### `trained_model.zip` (66.9 MB) contains:\n",
    "- **`best_model.pth`**: Optimal pre-trained model for inference.\n",
    "\n",
    "\ud83d\udcc2 **Installation**: Extract this zip file to the `./models/` folder.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb46dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import *\n",
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0df8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_file='./data/processed_data/singlePPD_full_bins_features.csv'\n",
    "df2_file='./data/processed_data/singlePPD_DockQ.csv'\n",
    "\n",
    "\n",
    "model_path=\"./models/best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81b2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import argparse\n",
    "\n",
    "# Numerical and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "lr=0.0005\n",
    "batch_size=512\n",
    "\n",
    "# num_epochs=1000\n",
    "# patience=1000\n",
    "\n",
    "\n",
    "num_epochs=20\n",
    "patience=20\n",
    "\n",
    "\n",
    "dropout=0.0\n",
    "\n",
    "\n",
    "\n",
    "df1=pd.read_csv(df1_file)\n",
    "\n",
    "\n",
    "\n",
    "df_train=df1[df1['data_class']!='test']\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "identical_columns_count = sum(df_train.nunique() == 1)\n",
    "\n",
    "# print(f\"Number of columns with all identical values: {identical_columns_count}\")\n",
    "#####\n",
    "\n",
    "\n",
    "identical_columns =df_train.columns[df_train.nunique() == 1]\n",
    "\n",
    "# Print the names of these columns\n",
    "# print(\"Columns with all identical values:\", identical_columns.tolist())\n",
    "\n",
    "# Remove these columns from the DataFrame\n",
    "df1= df1.drop(columns=identical_columns)\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "df2=pd.read_csv(df2_file)\n",
    "\n",
    "\n",
    "df2=df2[['pdb_id','af_model_id','af_confidence','pdb2sql_DockQ','data_class']]\n",
    "\n",
    "df1=pd.merge(df1,df2,on=['pdb_id','af_model_id','pdb2sql_DockQ','data_class'],how='inner')\n",
    "\n",
    "\n",
    "df1_for_eval=df1\n",
    "# print(df1.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_train=df1[df1['data_class']=='train']\n",
    "\n",
    "\n",
    "df_val=df1[df1['data_class']=='valid']\n",
    "df_test=df1[df1['data_class']=='test']\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "list_train=list(set(df_train['pdb_id'].to_list()))\n",
    "list_val=list(set(df_val['pdb_id'].to_list()))\n",
    "list_test=list(set(df_test['pdb_id'].to_list()))\n",
    "# print(len(list_train),len(list_val),len(list_test))\n",
    "for i in list_train:\n",
    "    if (i in list_val) or (i in list_test):\n",
    "        print(i)\n",
    "for i in list_test:\n",
    "    if (i in list_val) or (i in list_train):\n",
    "        print(i)\n",
    "\n",
    "################################################################################################################\n",
    "import numpy as np\n",
    "filtration_values = [f\"{x:.1f}\" if x.is_integer() else f\"{x:.2f}\".rstrip(\"0\") for x in np.arange(2.0, 10.25, 0.25)]\n",
    "filtration_values\n",
    "# Generate persistent feature names for each filtration value\n",
    "persistent_features = []\n",
    "for filtration in filtration_values:\n",
    "    persistent_features += [f\"persistent_{filtration}_{str(i+1).zfill(2)}\" for i in range(72)]  # Adjust 72 to the actual count if different\n",
    "\n",
    "# Generate static feature names\n",
    "static_features = [f\"static_{str(i+1).zfill(2)}\" for i in range(378)]  # Adjust 378 to the actual count if different\n",
    "\n",
    "# Combine both lists\n",
    "combined_feature_name_list = persistent_features + static_features\n",
    "\n",
    "\n",
    "feature_name_list=[\n",
    "                \n",
    "\n",
    "\n",
    "                   'pdb2sql_DockQ'] +combined_feature_name_list\n",
    "\n",
    "valid_columns = [col for col in feature_name_list if col in df_train.columns]\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "# Subset the DataFrame with the valid columns\n",
    "df_train = df_train[valid_columns]\n",
    "df_val = df_val[valid_columns]\n",
    "df_test = df_test[valid_columns]\n",
    "\n",
    "# print(df_train.shape,df_val.shape,df_test.shape)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "def get_features_and_target(df):\n",
    "    y = df['pdb2sql_DockQ'].values  # Select the 'pdb2sql_DockQ' column as the target\n",
    "    X = df.drop(columns=['pdb2sql_DockQ']).values  # Drop the 'pdb2sql_DockQ' column to get the features\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66e4d4",
   "metadata": {},
   "source": [
    "# Standardized input features before conducting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2032c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Extract features and targets\n",
    "X_train, y_train = get_features_and_target(df_train)\n",
    "X_val, y_val = get_features_and_target(df_val)\n",
    "X_test, y_test = get_features_and_target(df_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Convert arrays to PyTorch tensors and create datasets\n",
    "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=1000000)\n",
    "test_loader = DataLoader(test_data, batch_size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a652ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 2646\n",
    "neurons1 = 2048     \n",
    "neurons2 = 2048  \n",
    "neurons3 = 2048 \n",
    "neurons4 = 2048 \n",
    "\n",
    "dropout = dropout   \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a03b0",
   "metadata": {},
   "source": [
    "# Inference on the pre-trained model:\n",
    "## The example model used in this step was trained and saved in tutorial_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcea1de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation inference completed. Results saved to 'val_inference_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert X_val and y_val to tensors\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1000000)\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = TopoDockQ(input_dim, neurons1, neurons2, neurons3, neurons4, dropout).to(device)\n",
    "best_model.load_state_dict(torch.load(model_path))\n",
    "best_model.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        predictions.append(outputs.cpu())\n",
    "        true_values.append(targets.cpu())\n",
    "\n",
    "# Concatenate results\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "true_values = torch.cat(true_values).numpy()\n",
    "\n",
    "# Save predictions to CSV\n",
    "import pandas as pd\n",
    "\n",
    "val_results_df = pd.DataFrame({\n",
    "    'True_DockQ': true_values.flatten(),\n",
    "    'Predicted_DockQ(p-DockQ)': predictions.flatten()\n",
    "})\n",
    "val_results_df.to_csv(\"val_inference_results.csv\", index=False)\n",
    "\n",
    "print(\"Validation inference completed. Results saved to 'val_inference_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f8bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlms2023",
   "language": "python",
   "name": "mlms2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
